--per_device_train_batch_size 4
--per_device_eval_batch_size 16
--learning_rate 5e-5
--weight_decay 0.0
--optim adamw_torch
--adam_beta1 0.9
--adam_beta2 0.999
--adam_epsilon 1e-8
--max_grad_norm 1.0
--num_train_epochs 1
--lr_scheduler_type linear
--warmup_steps 0
--warmup_ratio 0.0
--log_level info
--logging_strategy steps
--logging_steps 100
--save_strategy no
--metric_for_best_model loss
--eval_do_concat_batches no
